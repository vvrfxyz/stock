import os
import sys
import time
from datetime import datetime, timedelta, date
from concurrent.futures import ThreadPoolExecutor, as_completed
from collections import Counter

from loguru import logger
from tqdm import tqdm

# --- è·¯å¾„è®¾ç½® ---
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
if project_root not in sys.path:
    sys.path.insert(0, project_root)
# --- è·¯å¾„è®¾ç½®ç»“æŸ ---

from db_manager import DatabaseManager
from data_models.models import Security, DailyPrice
from data_sources.polygon_source import PolygonSource
from utils.key_rate_limiter import KeyRateLimiter

# --- é…ç½®åŒº ---
MAX_CONCURRENT_WORKERS = 10
POLYGON_RATE_LIMIT = 5
POLYGON_RATE_SECONDS = 60


# ... (setup_logging, get_dates_to_process å‡½æ•°ä¿æŒä¸å˜) ...
def setup_logging():
    """é…ç½® Loguru æ—¥å¿—è®°å½•å™¨"""
    logger.remove()
    log_format = (
        "<green>{time:YYYY-MM-DD HH:mm:ss}</green> | "
        "<level>{level: <8}</level> | "
        "<cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> - <level>{message}</level>"
    )
    logger.add(sys.stderr, level="INFO", format=log_format)
    log_dir = os.path.join(project_root, "logs")
    os.makedirs(log_dir, exist_ok=True)
    logger.add(os.path.join(log_dir, f"update_grouped_daily_{{time}}.log"), rotation="10 MB", retention="10 days",
               level="DEBUG")
    logger.info("æ—¥å¿—è®°å½•å™¨è®¾ç½®å®Œæˆã€‚")


def get_dates_to_process(start_str: str, end_str: str) -> list[date]:
    """æ ¹æ®èµ·æ­¢æ—¥æœŸå­—ç¬¦ä¸²ï¼Œç”Ÿæˆä¸€ä¸ªæ—¥æœŸå¯¹è±¡åˆ—è¡¨ã€‚"""
    try:
        start = date.fromisoformat(start_str)
        end = date.fromisoformat(end_str)
    except ValueError:
        logger.critical(f"æ—¥æœŸæ ¼å¼é”™è¯¯ã€‚è¯·ä½¿ç”¨ 'YYYY-MM-DD' æ ¼å¼ã€‚")
        return []
    if start > end:
        logger.warning(f"èµ·å§‹æ—¥æœŸ {start_str} åœ¨ç»“æŸæ—¥æœŸ {end_str} ä¹‹åï¼Œä¸æ‰§è¡Œä»»ä½•æ“ä½œã€‚")
        return []
    dates = [start + timedelta(days=x) for x in range((end - start).days + 1)]
    return dates


def process_date(target_date: date, polygon_source: PolygonSource, db_manager: DatabaseManager,
                 symbol_to_id_map: dict) -> tuple[str, str, int]:
    """
    ã€æ–°é€»è¾‘ã€‘å¤„ç†å•ä¸ªæ—¥æœŸçš„è¡Œæƒ…æ•°æ®ï¼šæŸ¥è¯¢ -> APIè·å– -> å†…å­˜ä¸­ä¿®æ”¹ -> æ‰¹é‡æ›´æ–°ã€‚
    """
    date_str = target_date.strftime('%Y-%m-%d')
    try:
        # 1. æŸ¥è¯¢ï¼šä»æ•°æ®åº“åŠ è½½å½“å¤©æ‰€æœ‰éœ€è¦æ›´æ–°çš„ DailyPrice ORM å¯¹è±¡
        with db_manager.get_session() as session:
            # å°†è®°å½•åŠ è½½åˆ°ä¸€ä¸ªå­—å…¸ä¸­ï¼Œä»¥ security_id ä¸ºé”®ï¼Œæ–¹ä¾¿å¿«é€ŸæŸ¥æ‰¾
            existing_records_map = {
                record.security_id: record
                for record in session.query(DailyPrice).filter(DailyPrice.date == target_date)
            }

        if not existing_records_map:
            logger.info(f"[{date_str}] æ•°æ®åº“ä¸­æ²¡æœ‰ä»»ä½•ä»·æ ¼è®°å½•ï¼Œè·³è¿‡æ­¤æ—¥æœŸã€‚")
            return date_str, "SKIPPED_NO_EXISTING_DATA", 0

        # 2. APIè·å–ï¼šä» Polygon æ‹‰å–æ•°æ®
        daily_aggs = polygon_source.get_grouped_daily_data(date_str)
        if not daily_aggs:
            logger.info(f"[{date_str}] Polygon API æœªè¿”å›ä»»ä½•è¡Œæƒ…æ•°æ®ã€‚")
            return date_str, "SUCCESS_NO_API_DATA", 0

        # 3. å†…å­˜ä¸­ä¿®æ”¹ï¼šéå†APIæ•°æ®ï¼Œæ›´æ–°ä»æ•°æ®åº“æŸ¥å‡ºçš„ORMå¯¹è±¡
        records_to_update = []
        for agg in daily_aggs:
            symbol = agg.get('T', '').lower()
            security_id = symbol_to_id_map.get(symbol)

            # æ£€æŸ¥è¿™æ¡APIæ•°æ®æ˜¯å¦å¯¹åº”æˆ‘ä»¬å·²æœ‰çš„è®°å½•
            if security_id in existing_records_map:
                # è·å–å¾…æ›´æ–°çš„ ORM å¯¹è±¡
                record_to_modify = existing_records_map[security_id]

                # è®¡ç®—æˆäº¤é¢
                volume = agg.get('v')
                vwap = agg.get('vw')
                turnover = (volume * vwap) if volume is not None and vwap is not None else None

                # åœ¨å†…å­˜ä¸­ç›´æ¥ä¿®æ”¹å¯¹è±¡çš„å±æ€§
                record_to_modify.open = agg.get('o')
                record_to_modify.high = agg.get('h')
                record_to_modify.low = agg.get('l')
                record_to_modify.close = agg.get('c')
                record_to_modify.volume = volume
                record_to_modify.vwap = vwap
                record_to_modify.turnover = turnover
                # **å…³é”®**: turnover_rate å’Œ adj_factor ç­‰å…¶ä»–å­—æ®µä¿æŒåŸæ ·ï¼Œå› ä¸ºæˆ‘ä»¬æ²¡æœ‰åŠ¨å®ƒä»¬

                records_to_update.append(record_to_modify)

        if not records_to_update:
            logger.info(f"[{date_str}] APIæ•°æ®ä¸æ•°æ®åº“ç°æœ‰è®°å½•æ— äº¤é›†ï¼Œæ— éœ€æ›´æ–°ã€‚")
            return date_str, "SUCCESS_NO_INTERSECTION", 0

        # 4. æ‰¹é‡æ›´æ–°ï¼šè°ƒç”¨æ–°çš„DBæ–¹æ³•ï¼Œå°†ä¿®æ”¹åçš„å¯¹è±¡åˆ—è¡¨ä¸€æ¬¡æ€§æäº¤
        rows_affected = db_manager.bulk_update_records(records_to_update)

        logger.success(f"[{date_str}] æˆåŠŸåˆ·æ–° {rows_affected} æ¡æ—¥çº¿æ•°æ®ã€‚")
        return date_str, "SUCCESS", rows_affected

    except Exception as e:
        logger.error(f"å¤„ç†æ—¥æœŸ {date_str} çš„è¡Œæƒ…æ•°æ®æ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}", exc_info=True)
        return date_str, "ERROR", 0


def main():
    """è„šæœ¬ä¸»å…¥å£"""
    start_time = time.monotonic()
    setup_logging()

    # ç¡¬ç¼–ç æ—¥æœŸèŒƒå›´ä»¥æ»¡è¶³ä½ çš„ç‰¹å®šéœ€æ±‚
    class Args:
        start_date = "2023-06-29"
        end_date = "2025-06-27"
        workers = MAX_CONCURRENT_WORKERS

    args = Args()
    logger.info(f"è„šæœ¬å°†åˆ·æ–°ä» {args.start_date} åˆ° {args.end_date} çš„æ•°æ®ã€‚")

    db_manager = None
    try:
        # --- åˆå§‹åŒ–å…±äº«èµ„æº ---
        api_keys_str = os.getenv("POLYGON_API_KEYS")
        if not api_keys_str:
            raise ValueError("ç¯å¢ƒå˜é‡ POLYGON_API_KEYS æœªè®¾ç½®ã€‚")
        api_keys = [key.strip() for key in api_keys_str.split(',') if key.strip()]

        rate_limiter = KeyRateLimiter(keys=api_keys, rate_limit=POLYGON_RATE_LIMIT, per_seconds=POLYGON_RATE_SECONDS)
        polygon_source = PolygonSource(rate_limiter=rate_limiter)
        db_manager = DatabaseManager()

        logger.info("æ­£åœ¨ä»æ•°æ®åº“åŠ è½½ 'symbol -> security_id' æ˜ å°„...")
        with db_manager.get_session() as session:
            securities = session.query(Security.id, Security.symbol).all()
            symbol_to_id_map = {s.symbol.lower(): s.id for s in securities}
        logger.success(f"æˆåŠŸåŠ è½½ {len(symbol_to_id_map)} ä¸ªè‚¡ç¥¨çš„IDæ˜ å°„ã€‚")

        dates_to_process = get_dates_to_process(args.start_date, args.end_date)
        if not dates_to_process: return

        total_count = len(dates_to_process)
        logger.info(f"å…±æ‰¾åˆ° {total_count} ä¸ªæ—¥æœŸéœ€è¦å¤„ç†ã€‚å°†ä½¿ç”¨æœ€å¤š {args.workers} ä¸ªå¹¶å‘çº¿ç¨‹ã€‚")

        results_counter = Counter()
        total_records_updated = 0
        with ThreadPoolExecutor(max_workers=args.workers) as executor:
            future_to_date = {
                executor.submit(process_date, dt, polygon_source, db_manager, symbol_to_id_map): dt
                for dt in dates_to_process
            }
            for future in tqdm(as_completed(future_to_date), total=total_count, desc="åˆ·æ–°æ¯æ—¥èšåˆè¡Œæƒ…"):
                try:
                    date_str, status, records_count = future.result()
                    results_counter[status] += 1
                    total_records_updated += records_count
                except Exception as exc:
                    dt = future_to_date[future]
                    logger.error(f"ä»»åŠ¡ {dt.strftime('%Y-%m-%d')} ç”Ÿæˆäº†æœªæ•è·çš„å¼‚å¸¸: {exc}")
                    results_counter["FATAL_ERROR"] += 1

        logger.info("--- ä»»åŠ¡æ‰§è¡Œç»Ÿè®¡ ---")
        for status, count in results_counter.items():
            logger.info(f"  {status}: {count} å¤©")
        logger.info(f"æ€»å…±åˆ·æ–°è®°å½•æ•°: {total_records_updated}")
        logger.info("----------------------")

    except ValueError as e:
        logger.critical(f"åˆå§‹åŒ–å¤±è´¥: {e}")
    except Exception as e:
        logger.critical(f"è„šæœ¬æ‰§è¡Œè¿‡ç¨‹ä¸­é‡åˆ°æœªå¤„ç†çš„ä¸¥é‡é”™è¯¯: {e}", exc_info=True)
    finally:
        if db_manager:
            db_manager.close()
        end_time = time.monotonic()
        logger.info(f"ğŸ è„šæœ¬æ‰§è¡Œå®Œæ¯•ã€‚æ€»è€—æ—¶: {timedelta(seconds=end_time - start_time)}")


if __name__ == "__main__":
    main()
