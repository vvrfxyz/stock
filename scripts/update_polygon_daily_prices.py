# scripts/update_polygon_daily_prices.py
import os
import sys
import time
import argparse
from datetime import datetime, timedelta, date
from concurrent.futures import ThreadPoolExecutor, as_completed
from collections import Counter

import pandas as pd
from loguru import logger
from sqlalchemy import or_, func
from tqdm import tqdm

# --- è·¯å¾„è®¾ç½® ---
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
if project_root not in sys.path:
    sys.path.insert(0, project_root)
# --- è·¯å¾„è®¾ç½®ç»“æŸ ---

from db_manager import DatabaseManager
from data_models.models import Security
from data_sources.polygon_source import PolygonSource
from utils.key_rate_limiter import KeyRateLimiter

# --- é…ç½®åŒº ---
# Polygon API è¾ƒä¸ºå¥å£®ï¼Œå¹¶å‘æ•°å¯ä»¥è®¾ç½®å¾—é«˜ä¸€äº›
MAX_CONCURRENT_WORKERS = 15
# æ›´æ–°æ£€æŸ¥å‘¨æœŸï¼Œä¸ em è„šæœ¬ä¿æŒä¸€è‡´
INCREMENTAL_CHECK_DAYS = 2
# Polygon API çš„é€Ÿç‡é™åˆ¶é…ç½®
POLYGON_RATE_LIMIT = 5
POLYGON_RATE_SECONDS = 60


def setup_logging():
    """é…ç½® Loguru æ—¥å¿—è®°å½•å™¨"""
    logger.remove()
    log_format = (
        "<green>{time:YYYY-MM-DD HH:mm:ss}</green> | "
        "<level>{level: <8}</level> | "
        "<cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> - <level>{message}</level>"
    )
    logger.add(sys.stderr, level="INFO", format=log_format)
    log_dir = os.path.join(project_root, "logs")
    os.makedirs(log_dir, exist_ok=True)
    logger.add(os.path.join(log_dir, f"update_polygon_prices_{{time}}.log"), rotation="10 MB", retention="10 days",
               level="DEBUG")
    logger.info("æ—¥å¿—è®°å½•å™¨è®¾ç½®å®Œæˆã€‚")


def create_parser() -> argparse.ArgumentParser:
    """åˆ›å»ºå¹¶è¿”å› ArgumentParser å¯¹è±¡ã€‚"""
    parser = argparse.ArgumentParser(
        description="ä½¿ç”¨ Polygon.io API è·å–å†å²æ—¥çº¿æ•°æ®å¹¶å­˜å‚¨åˆ°æ•°æ®åº“ã€‚",
        formatter_class=argparse.RawTextHelpFormatter
    )
    # æ ‡è¯†ç¬¦æ”¹ä¸º 'symbols'
    parser.add_argument('symbols', nargs='*',
                        help="è¦æ›´æ–°çš„è‚¡ç¥¨ä»£ç åˆ—è¡¨ (e.g., 'aapl', 'nvda')ã€‚å¦‚æœä¸ºç©ºï¼Œåˆ™ä¾èµ–å…¶ä»–æ ‡å¿—ã€‚")
    parser.add_argument('--market', type=str, default='US', help="ä»…å¤„ç†æŒ‡å®šå¸‚åœºçš„è‚¡ç¥¨ (é»˜è®¤: 'US')ã€‚")
    parser.add_argument('--full-refresh', action='store_true',
                        help="å¼ºåˆ¶å¯¹é€‰å®šèŒƒå›´å†…çš„æ‰€æœ‰è‚¡ç¥¨è¿›è¡Œå…¨é‡åˆ·æ–°ï¼Œå¿½ç•¥å…¶ç°æœ‰æ•°æ®ã€‚")
    parser.add_argument('--limit', type=int, default=0, help="é™åˆ¶å¤„ç†çš„è‚¡ç¥¨æ•°é‡ï¼Œç”¨äºæµ‹è¯•ã€‚0è¡¨ç¤ºä¸é™åˆ¶ã€‚")
    parser.add_argument('--workers', type=int, default=MAX_CONCURRENT_WORKERS,
                        help=f"å¹¶å‘æ‰§è¡Œçš„çº¿ç¨‹æ•° (é»˜è®¤: {MAX_CONCURRENT_WORKERS})ã€‚")
    return parser


def get_securities_to_update(db_manager: DatabaseManager, args: argparse.Namespace) -> list[Security]:
    """æ ¹æ®å‘½ä»¤è¡Œå‚æ•°ï¼Œä»æ•°æ®åº“æŸ¥è¯¢éœ€è¦æ›´æ–°æ—¥çº¿æ•°æ®çš„è¯åˆ¸åˆ—è¡¨ã€‚"""
    with db_manager.get_session() as session:
        query = session.query(Security).filter(
            Security.is_active == True,
            func.upper(Security.market) == args.market.upper()
        )

        if args.symbols:
            symbols_lower = [s.lower() for s in args.symbols]
            query = query.filter(Security.symbol.in_(symbols_lower))

        if not args.full_refresh:
            # å¢é‡æ¨¡å¼ï¼šåªé€‰æ‹©é‚£äº›æ•°æ®ä¸æ˜¯æœ€æ–°çš„è‚¡ç¥¨
            latest_required_date = date.today() - timedelta(days=INCREMENTAL_CHECK_DAYS)
            query = query.filter(
                or_(
                    Security.price_data_latest_date.is_(None),
                    Security.price_data_latest_date < latest_required_date
                )
            )

        # ä¼˜å…ˆå¤„ç†æ²¡æœ‰æ•°æ®çš„
        query = query.order_by(Security.price_data_latest_date.asc().nulls_first())

        if args.limit > 0:
            query = query.limit(args.limit)

        return query.all()


def process_security(security: Security, polygon_source: PolygonSource, db_manager: DatabaseManager,
                     full_refresh: bool) -> tuple[str, str, int]:
    """
    å¤„ç†å•ä¸ªè‚¡ç¥¨çš„æ—¥çº¿æ•°æ®ï¼šAPIè·å– -> æ•°æ®æ¸…æ´— -> DBå­˜å‚¨ -> æ›´æ–°æ—¶é—´æˆ³ã€‚
    """
    symbol = security.symbol
    is_full_run = False

    try:
        # 1. ç¡®å®šè·å–æ•°æ®çš„èµ·æ­¢æ—¥æœŸ
        end_date = date.today().strftime('%Y-%m-%d')
        if full_refresh or security.price_data_latest_date is None:
            # Polygon å…è´¹ç‰ˆæœ€å¤šæä¾›çº¦2å¹´çš„æ•°æ®ï¼Œä½†ä¸ºç¡®ä¿è·å–æ‰€æœ‰å¯ç”¨æ•°æ®ï¼Œä»ä¸€ä¸ªå¾ˆæ—©çš„æ—¥æœŸå¼€å§‹
            start_date = '1970-01-01'
            is_full_run = True
            logger.debug(f"[{symbol}] å…¨é‡æ›´æ–°ï¼Œèµ·å§‹æ—¥æœŸ: {start_date}")
        else:
            start_date = (security.price_data_latest_date + timedelta(days=1)).strftime('%Y-%m-%d')
            logger.debug(f"[{symbol}] å¢é‡æ›´æ–°ï¼Œèµ·å§‹æ—¥æœŸ: {start_date}")

        if start_date > end_date:
            logger.info(f"[{symbol}] æ•°æ®å·²æ˜¯æœ€æ–°ï¼Œæ— éœ€æ›´æ–°ã€‚")
            return symbol, "SUCCESS_UP_TO_DATE", 0

        # 2. è°ƒç”¨ PolygonSource è·å–æ•°æ®
        df = polygon_source.get_historical_data(symbol=symbol, start=start_date, end=end_date)

        if df.empty:
            logger.info(f"[{symbol}] åœ¨æ—¶é—´èŒƒå›´ {start_date}-{end_date} æœªè·å–åˆ°æ–°æ•°æ®ã€‚")
            # å³ä½¿æ²¡æ•°æ®ï¼Œå¦‚æœæ˜¯å¢é‡æ¨¡å¼ï¼Œä¹Ÿæ›´æ–°æ—¶é—´æˆ³åˆ°æ˜¨å¤©ï¼Œé¿å…é¢‘ç¹æŸ¥è¯¢
            if not full_refresh:
                db_manager.update_security_price_latest_date(security.id, date.today() - timedelta(days=1), is_full_run)
            return symbol, "SUCCESS_NO_NEW_DATA", 0

        # 3. æ•°æ®æ¸…æ´—å’Œæ ¼å¼åŒ– (polygon_source å·²åŸºæœ¬å®Œæˆï¼Œæ­¤å¤„ä¸»è¦æ˜¯å‡†å¤‡å…¥åº“)
        df['date'] = df.index
        df.reset_index(drop=True, inplace=True)

        # 4. å‡†å¤‡å…¥åº“æ•°æ®
        df['security_id'] = security.id
        # Polygon æ—¥çº¿èšåˆä¸æä¾›æ¢æ‰‹ç‡ï¼Œå°†å…¶è®¾ç½®ä¸º None
        df['turnover_rate'] = None
        # é‡å‘½åå­—æ®µä»¥åŒ¹é…æ•°æ®åº“æ¨¡å‹
        df.rename(columns={
            'Open': 'open', 'High': 'high', 'Low': 'low', 'Close': 'close', 'Volume': 'volume'
        }, inplace=True)

        required_cols = ['security_id', 'date', 'open', 'high', 'low', 'close', 'volume', 'turnover', 'vwap',
                         'turnover_rate']
        price_data = df[required_cols].to_dict('records')

        # 5. å­˜å‚¨åˆ°æ•°æ®åº“
        rows_affected = db_manager.upsert_daily_prices(price_data)

        # 6. æ›´æ–° Security è¡¨çš„æ—¶é—´æˆ³
        latest_date_in_df = df['date'].max()
        db_manager.update_security_price_latest_date(security.id, latest_date_in_df, is_full_run)

        logger.success(f"[{symbol}] æˆåŠŸåŒæ­¥ {len(price_data)} æ¡æ—¥çº¿æ•°æ®ï¼Œæœ€æ–°æ—¥æœŸ: {latest_date_in_df}ã€‚")
        return symbol, "SUCCESS", len(price_data)

    except Exception as e:
        logger.error(f"å¤„ç†è‚¡ç¥¨ {symbol} æ—¥çº¿æ•°æ®æ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}", exc_info=True)
        return symbol, "ERROR", 0


def main():
    """è„šæœ¬ä¸»å…¥å£"""
    start_time = time.monotonic()
    setup_logging()
    parser = create_parser()
    args = parser.parse_args()

    db_manager = None
    try:
        # --- åˆå§‹åŒ–å…±äº«èµ„æº ---
        api_keys_str = os.getenv("POLYGON_API_KEYS")
        if not api_keys_str:
            raise ValueError("ç¯å¢ƒå˜é‡ POLYGON_API_KEYS æœªè®¾ç½®ã€‚")
        api_keys = [key.strip() for key in api_keys_str.split(',') if key.strip()]

        rate_limiter = KeyRateLimiter(
            keys=api_keys,
            rate_limit=POLYGON_RATE_LIMIT,
            per_seconds=POLYGON_RATE_SECONDS
        )

        polygon_source = PolygonSource(rate_limiter=rate_limiter)
        db_manager = DatabaseManager()
        # --- åˆå§‹åŒ–ç»“æŸ ---

        securities_to_process = get_securities_to_update(db_manager, args)

        if not securities_to_process:
            logger.success("âœ… æ ¹æ®æ‚¨çš„æ¡ä»¶ï¼Œæ²¡æœ‰æ‰¾åˆ°éœ€è¦æ›´æ–°æ—¥çº¿æ•°æ®çš„è‚¡ç¥¨ã€‚ä»»åŠ¡å®Œæˆã€‚")
            return

        total_count = len(securities_to_process)
        logger.info(f"å…±æ‰¾åˆ° {total_count} æ”¯è‚¡ç¥¨éœ€è¦ä» Polygon æ›´æ–°æ—¥çº¿æ•°æ®ã€‚å°†ä½¿ç”¨æœ€å¤š {args.workers} ä¸ªå¹¶å‘çº¿ç¨‹ã€‚")
        if args.full_refresh:
            logger.warning("âš ï¸ å·²å¯ç”¨ --full-refresh æ¨¡å¼ï¼Œå°†å¯¹æ‰€æœ‰é€‰å®šè‚¡ç¥¨è¿›è¡Œå…¨é‡æ•°æ®åˆ·æ–°ï¼")

        results_counter = Counter()
        total_rows_synced = 0

        with ThreadPoolExecutor(max_workers=args.workers) as executor:
            future_to_security = {
                executor.submit(process_security, security, polygon_source, db_manager, args.full_refresh): security
                for security in securities_to_process
            }

            for future in tqdm(as_completed(future_to_security), total=total_count, desc="æ›´æ–°è‚¡ç¥¨æ—¥çº¿(Polygon)"):
                try:
                    symbol, status, rows_count = future.result()
                    results_counter[status] += 1
                    total_rows_synced += rows_count
                except Exception as exc:
                    security = future_to_security[future]
                    logger.error(f"ä»»åŠ¡ {security.symbol} ç”Ÿæˆäº†æœªæ•è·çš„å¼‚å¸¸: {exc}", exc_info=True)
                    results_counter["FATAL_ERROR"] += 1

        logger.info("--- ä»»åŠ¡æ‰§è¡Œç»Ÿè®¡ ---")
        logger.info(f"  æˆåŠŸ (æœ‰æ–°æ•°æ®): {results_counter['SUCCESS']}")
        logger.info(f"  æˆåŠŸ (æ— æ–°æ•°æ®): {results_counter['SUCCESS_NO_NEW_DATA']}")
        logger.info(f"  æˆåŠŸ (å·²æ˜¯æœ€æ–°): {results_counter['SUCCESS_UP_TO_DATE']}")
        logger.info(f"  é”™è¯¯: {results_counter['ERROR'] + results_counter['FATAL_ERROR']}")
        logger.info(f"  æ€»å…±åŒæ­¥æ•°æ®è¡Œæ•°: {total_rows_synced}")
        logger.info("----------------------")

    except ValueError as e:
        logger.critical(f"åˆå§‹åŒ–å¤±è´¥: {e}")
    except Exception as e:
        logger.critical(f"è„šæœ¬æ‰§è¡Œè¿‡ç¨‹ä¸­é‡åˆ°æœªå¤„ç†çš„ä¸¥é‡é”™è¯¯: {e}", exc_info=True)
    finally:
        if db_manager:
            db_manager.close()
        end_time = time.monotonic()
        logger.info(f"ğŸ è„šæœ¬æ‰§è¡Œå®Œæ¯•ã€‚æ€»è€—æ—¶: {timedelta(seconds=end_time - start_time)}")


if __name__ == "__main__":
    main()
