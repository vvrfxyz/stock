# scripts/update_em_us_daily_prices.py (æ­£ç¡®ç‰ˆæœ¬)

import os
import sys
import time
import argparse
import random
from datetime import datetime, timedelta, date
from concurrent.futures import ThreadPoolExecutor, as_completed
from collections import Counter

import akshare
import pandas as pd
from loguru import logger
from sqlalchemy import or_, func
from tqdm import tqdm

# --- è·¯å¾„è®¾ç½® ---
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
if project_root not in sys.path:
    sys.path.insert(0, project_root)
# --- è·¯å¾„è®¾ç½®ç»“æŸ ---

from db_manager import DatabaseManager
from data_models.models import Security

# --- é…ç½®åŒº ---
# Akshare æŠ“å–ç½‘é¡µï¼Œå¹¶å‘ä¸å®œè¿‡é«˜ï¼Œä¸”éœ€è¦éšæœºå»¶æ—¶ä»¥é˜²å°ç¦
MAX_CONCURRENT_WORKERS = 4
# é»˜è®¤å¢é‡æ›´æ–°æ—¶ï¼Œæ£€æŸ¥æœ€è¿‘ä¸¤å¤©çš„æ•°æ®æ˜¯å¦å·²åŒæ­¥
INCREMENTAL_CHECK_DAYS = 2


def setup_logging():
    """é…ç½® Loguru æ—¥å¿—è®°å½•å™¨"""
    logger.remove()
    log_format = (
        "<green>{time:YYYY-MM-DD HH:mm:ss}</green> | "
        "<level>{level: <8}</level> | "
        "<cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> - <level>{message}</level>"
    )
    logger.add(sys.stderr, level="INFO", format=log_format)
    log_dir = os.path.join(project_root, "logs")
    os.makedirs(log_dir, exist_ok=True)
    logger.add(os.path.join(log_dir, f"update_em_us_prices_{{time}}.log"), rotation="10 MB", retention="10 days",
               level="DEBUG")
    logger.info("æ—¥å¿—è®°å½•å™¨è®¾ç½®å®Œæˆã€‚")


def create_parser() -> argparse.ArgumentParser:
    """åˆ›å»ºå¹¶è¿”å› ArgumentParser å¯¹è±¡ã€‚"""
    parser = argparse.ArgumentParser(
        description="ä½¿ç”¨ Akshare ä»ä¸œæ–¹è´¢å¯Œè·å–ç¾è‚¡å†å²æ—¥çº¿æ•°æ®å¹¶å­˜å‚¨åˆ°æ•°æ®åº“ã€‚",
        formatter_class=argparse.RawTextHelpFormatter
    )
    # æ ‡è¯†ç¬¦æ”¹ä¸º 'em_codes'
    parser.add_argument('em_codes', nargs='*',
                        help="è¦æ›´æ–°çš„è‚¡ç¥¨ä¸œæ–¹è´¢å¯Œä»£ç åˆ—è¡¨ (e.g., '105.NVDA')ã€‚å¦‚æœä¸ºç©ºï¼Œåˆ™ä¾èµ–å…¶ä»–æ ‡å¿—ã€‚")
    parser.add_argument('--market', type=str, default='US', help="ä»…å¤„ç†æŒ‡å®šå¸‚åœºçš„è‚¡ç¥¨ (é»˜è®¤: 'US')ã€‚")
    parser.add_argument('--full-refresh', action='store_true',
                        help="å¼ºåˆ¶å¯¹é€‰å®šèŒƒå›´å†…çš„æ‰€æœ‰è‚¡ç¥¨è¿›è¡Œå…¨é‡åˆ·æ–°ï¼Œå¿½ç•¥å…¶ç°æœ‰æ•°æ®ã€‚")
    parser.add_argument('--limit', type=int, default=0, help="é™åˆ¶å¤„ç†çš„è‚¡ç¥¨æ•°é‡ï¼Œç”¨äºæµ‹è¯•ã€‚0è¡¨ç¤ºä¸é™åˆ¶ã€‚")
    parser.add_argument('--workers', type=int, default=MAX_CONCURRENT_WORKERS,
                        help=f"å¹¶å‘æ‰§è¡Œçš„çº¿ç¨‹æ•° (é»˜è®¤: {MAX_CONCURRENT_WORKERS})ã€‚")
    return parser


def get_securities_to_update(db_manager: DatabaseManager, args: argparse.Namespace) -> list[Security]:
    """æ ¹æ®å‘½ä»¤è¡Œå‚æ•°ï¼Œä»æ•°æ®åº“æŸ¥è¯¢éœ€è¦æ›´æ–°æ—¥çº¿æ•°æ®çš„è¯åˆ¸åˆ—è¡¨ã€‚"""
    with db_manager.get_session() as session:
        query = session.query(Security).filter(
            Security.is_active == True,
            Security.em_code.isnot(None),  # å¿…é¡»æœ‰ em_code
            func.upper(Security.market) == args.market.upper()
        )

        if args.em_codes:
            query = query.filter(Security.em_code.in_(args.em_codes))

        if not args.full_refresh:
            # å¢é‡æ¨¡å¼ï¼šåªé€‰æ‹©é‚£äº›æ•°æ®ä¸æ˜¯æœ€æ–°çš„è‚¡ç¥¨
            latest_required_date = date.today() - timedelta(days=INCREMENTAL_CHECK_DAYS)
            query = query.filter(
                or_(
                    Security.price_data_latest_date.is_(None),
                    Security.price_data_latest_date < latest_required_date
                )
            )

        # ä¼˜å…ˆå¤„ç†æ²¡æœ‰æ•°æ®çš„
        query = query.order_by(Security.price_data_latest_date.asc().nulls_first())

        if args.limit > 0:
            query = query.limit(args.limit)

        return query.all()


def process_security(security: Security, db_manager: DatabaseManager, full_refresh: bool) -> tuple[str, str, int]:
    """
    å¤„ç†å•ä¸ªç¾è‚¡çš„æ—¥çº¿æ•°æ®ï¼šAPIè·å– -> æ•°æ®æ¸…æ´— -> DBå­˜å‚¨ -> æ›´æ–°æ—¶é—´æˆ³ã€‚
    """
    em_code = security.em_code
    is_full_run = False

    try:
        # 1. ç¡®å®šè·å–æ•°æ®çš„èµ·æ­¢æ—¥æœŸ
        end_date = datetime.now().strftime('%Y%m%d')
        if full_refresh or security.price_data_latest_date is None:
            start_date = '19700101'  # ä»ä¸€ä¸ªå¾ˆæ—©çš„æ—¥æœŸå¼€å§‹ï¼Œè·å–å…¨éƒ¨å†å²
            is_full_run = True
            logger.debug(f"[{em_code}] å…¨é‡æ›´æ–°ï¼Œèµ·å§‹æ—¥æœŸ: {start_date}")
        else:
            start_date = (security.price_data_latest_date + timedelta(days=1)).strftime('%Y%m%d')
            logger.debug(f"[{em_code}] å¢é‡æ›´æ–°ï¼Œèµ·å§‹æ—¥æœŸ: {start_date}")

        if start_date > end_date:
            logger.info(f"[{em_code}] æ•°æ®å·²æ˜¯æœ€æ–°ï¼Œæ— éœ€æ›´æ–°ã€‚")
            return em_code, "SUCCESS_UP_TO_DATE", 0

        # 2. è°ƒç”¨ Akshare API è·å–æ•°æ®ï¼ˆå¸¦éšæœºå»¶æ—¶ï¼‰
        time.sleep(random.uniform(1.0, 2.0))
        df = akshare.stock_us_hist(symbol=em_code, period="daily", start_date=start_date, end_date=end_date, adjust="")

        if df.empty:
            logger.info(f"[{em_code}] åœ¨æ—¶é—´èŒƒå›´ {start_date}-{end_date} æœªè·å–åˆ°æ–°æ•°æ®ã€‚")
            # å³ä½¿æ²¡æ•°æ®ï¼Œå¦‚æœæ˜¯å¢é‡æ¨¡å¼ï¼Œä¹Ÿæ›´æ–°æ—¶é—´æˆ³åˆ°æ˜¨å¤©ï¼Œé¿å…é¢‘ç¹æŸ¥è¯¢
            if not full_refresh:
                db_manager.update_security_price_latest_date(security.id, date.today() - timedelta(days=1), is_full_run)
            return em_code, "SUCCESS_NO_NEW_DATA", 0

        # 3. æ•°æ®æ¸…æ´—å’Œæ ¼å¼åŒ–
        df.rename(columns={
            'æ—¥æœŸ': 'date', 'å¼€ç›˜': 'open', 'æ”¶ç›˜': 'close', 'æœ€é«˜': 'high', 'æœ€ä½': 'low',
            'æˆäº¤é‡': 'volume', 'æˆäº¤é¢': 'turnover', 'æ¢æ‰‹ç‡': 'turnover_rate'
        }, inplace=True)

        df['date'] = pd.to_datetime(df['date']).dt.date
        # **é‡è¦**: å°†æ¢æ‰‹ç‡ä»ç™¾åˆ†æ¯”è½¬ä¸ºå°æ•°
        df['turnover_rate'] = pd.to_numeric(df['turnover_rate'], errors='coerce') / 100.0

        # 4. å‡†å¤‡å…¥åº“æ•°æ®
        df['security_id'] = security.id
        df['vwap'] = pd.Series(None, index=df.index, dtype='float64')

        required_cols = ['security_id', 'date', 'open', 'high', 'low', 'close', 'volume', 'turnover', 'turnover_rate',
                         'vwap']
        price_data = df[required_cols].to_dict('records')

        # 5. å­˜å‚¨åˆ°æ•°æ®åº“
        rows_affected = db_manager.upsert_daily_prices(price_data)

        # 6. æ›´æ–° Security è¡¨çš„æ—¶é—´æˆ³
        latest_date_in_df = df['date'].max()
        db_manager.update_security_price_latest_date(security.id, latest_date_in_df, is_full_run)

        logger.success(f"[{em_code}] æˆåŠŸåŒæ­¥ {len(price_data)} æ¡æ—¥çº¿æ•°æ®ï¼Œæœ€æ–°æ—¥æœŸ: {latest_date_in_df}ã€‚")
        return em_code, "SUCCESS", len(price_data)

    except Exception as e:
        logger.error(f"å¤„ç†è‚¡ç¥¨ {em_code} æ—¥çº¿æ•°æ®æ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}", exc_info=True)
        return em_code, "ERROR", 0


def main():
    """è„šæœ¬ä¸»å…¥å£"""
    start_time = time.monotonic()
    setup_logging()
    parser = create_parser()
    args = parser.parse_args()

    db_manager = None
    try:
        db_manager = DatabaseManager()
        securities_to_process = get_securities_to_update(db_manager, args)

        if not securities_to_process:
            logger.success("âœ… æ ¹æ®æ‚¨çš„æ¡ä»¶ï¼Œæ²¡æœ‰æ‰¾åˆ°éœ€è¦æ›´æ–°æ—¥çº¿æ•°æ®çš„è‚¡ç¥¨ã€‚ä»»åŠ¡å®Œæˆã€‚")
            return

        total_count = len(securities_to_process)
        logger.info(f"å…±æ‰¾åˆ° {total_count} æ”¯ç¾è‚¡éœ€è¦ä»ä¸œæ–¹è´¢å¯Œæ›´æ–°æ—¥çº¿æ•°æ®ã€‚å°†ä½¿ç”¨æœ€å¤š {args.workers} ä¸ªå¹¶å‘çº¿ç¨‹ã€‚")
        if args.full_refresh:
            logger.warning("âš ï¸ å·²å¯ç”¨ --full-refresh æ¨¡å¼ï¼Œå°†å¯¹æ‰€æœ‰é€‰å®šè‚¡ç¥¨è¿›è¡Œå…¨é‡æ•°æ®åˆ·æ–°ï¼")

        results_counter = Counter()
        total_rows_synced = 0

        with ThreadPoolExecutor(max_workers=args.workers) as executor:
            future_to_security = {
                executor.submit(process_security, security, db_manager, args.full_refresh): security
                for security in securities_to_process
            }

            for future in tqdm(as_completed(future_to_security), total=total_count, desc="æ›´æ–°ç¾è‚¡æ—¥çº¿(ä¸œæ–¹è´¢å¯Œ)"):
                try:
                    em_code, status, rows_count = future.result()
                    results_counter[status] += 1
                    total_rows_synced += rows_count
                except Exception as exc:
                    security = future_to_security[future]
                    logger.error(f"ä»»åŠ¡ {security.em_code} ç”Ÿæˆäº†æœªæ•è·çš„å¼‚å¸¸: {exc}")
                    results_counter["FATAL_ERROR"] += 1

        logger.info("--- ä»»åŠ¡æ‰§è¡Œç»Ÿè®¡ ---")
        logger.info(f"  æˆåŠŸ (æœ‰æ–°æ•°æ®): {results_counter['SUCCESS']}")
        logger.info(f"  æˆåŠŸ (æ— æ–°æ•°æ®): {results_counter['SUCCESS_NO_NEW_DATA']}")
        logger.info(f"  æˆåŠŸ (å·²æ˜¯æœ€æ–°): {results_counter['SUCCESS_UP_TO_DATE']}")
        logger.info(f"  é”™è¯¯: {results_counter['ERROR'] + results_counter['FATAL_ERROR']}")
        logger.info(f"  æ€»å…±åŒæ­¥æ•°æ®è¡Œæ•°: {total_rows_synced}")
        logger.info("----------------------")

    except Exception as e:
        logger.critical(f"è„šæœ¬æ‰§è¡Œè¿‡ç¨‹ä¸­é‡åˆ°æœªå¤„ç†çš„ä¸¥é‡é”™è¯¯: {e}", exc_info=True)
    finally:
        if db_manager:
            db_manager.close()
        end_time = time.monotonic()
        logger.info(f"ğŸ è„šæœ¬æ‰§è¡Œå®Œæ¯•ã€‚æ€»è€—æ—¶: {timedelta(seconds=end_time - start_time)}")


if __name__ == "__main__":
    main()
